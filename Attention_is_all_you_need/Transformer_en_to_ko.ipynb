{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0906927-cae9-487b-a670-b47df95a010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fb492f-d0b0-4544-9ff2-e57c79eaa5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\asy10\\\\Desktop\\\\수업\\\\기계학습특강\\\\논문구현'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fee5b8a9-ff17-453a-91e1-1a4b69701bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    " \n",
    "# 일상생활및구어체_영한_train_set.json' 파일을 읽어서 melon.csv 파일에 저장\n",
    "with open('./data/일상생활및구어체_영한_train_set.json', 'r', encoding = 'utf-8') as input_file, open('./data/train_set.csv', 'w', newline = '',encoding = 'utf-8') as output_file :\n",
    "    data = json.load(input_file)\n",
    "\n",
    "    f = csv.writer(output_file)\n",
    "    \n",
    "    # csv 파일에 header 추가\n",
    "    f.writerow(['en', 'ko'])\n",
    "    \n",
    "    for line in data['data']:\n",
    "        f.writerow([line['en'],line['ko']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf24596-8bab-48a0-a731-c46cca17fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일상생활및구어체_영한_valid_set.json' 파일을 읽어서 melon.csv 파일에 저장\n",
    "with open('./data/일상생활및구어체_영한_valid_set.json', 'r', encoding = 'utf-8') as input_file2, open('./data/test_set.csv', 'w', newline = '',encoding = 'utf-8') as output_file2 :\n",
    "    data2 = json.load(input_file2)\n",
    "\n",
    "    f = csv.writer(output_file2)\n",
    "    \n",
    "    # csv 파일에 header 추가\n",
    "    f.writerow(['en', 'ko'])\n",
    "    \n",
    "    for line in data2['data']:\n",
    "        f.writerow([line['en'],line['ko']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603ac859-a85e-4355-8acf-dc346e533846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be5cf12-9b1b-4740-acd1-3fb9988e550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "p_data = pd.read_csv('./data/train_set.csv',encoding='utf-8')\n",
    "test_data = pd.read_csv('./data/test_set.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a8889a-e135-49a8-bafd-89cfb8808d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm glad to hear that, and I hope you do consi...</td>\n",
       "      <td>그 말을 들으니 기쁘고, 저희와 거래하는 것을 고려해 주셨으면 합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm definitely thinking about it, but I have s...</td>\n",
       "      <td>확실히 생각하고 있습니다만, 몇 가지 여쭤보고 싶은 게 있어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In today's world, one in every five families h...</td>\n",
       "      <td>오늘날 세계 5가구 중 1가구는 고양이나 개 또는 둘 다를 키우고 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When you tell them, we'll take care of their c...</td>\n",
       "      <td>그들에게 말하면, 저희가 그 아이들을 돌볼 것입니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK, how about for swimming?</td>\n",
       "      <td>좋아요, 수영은 어떤가요?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  I'm glad to hear that, and I hope you do consi...   \n",
       "1  I'm definitely thinking about it, but I have s...   \n",
       "2  In today's world, one in every five families h...   \n",
       "3  When you tell them, we'll take care of their c...   \n",
       "4                        OK, how about for swimming?   \n",
       "\n",
       "                                           ko  \n",
       "0     그 말을 들으니 기쁘고, 저희와 거래하는 것을 고려해 주셨으면 합니다.  \n",
       "1         확실히 생각하고 있습니다만, 몇 가지 여쭤보고 싶은 게 있어요.  \n",
       "2  오늘날 세계 5가구 중 1가구는 고양이나 개 또는 둘 다를 키우고 있습니다.  \n",
       "3               그들에게 말하면, 저희가 그 아이들을 돌볼 것입니다.  \n",
       "4                              좋아요, 수영은 어떤가요?  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf767f7d-6940-40f9-b341-59388fb4b58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200307"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3306f44-0e0a-41db-b9e0-dd2004fd2331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150038"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1aabe2-6ce3-4bd3-8f63-2ebfb345784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "from torchtext.legacy import data \n",
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76662beb-1e59-478e-8394-241af0d6c331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abd5d9-0b63-4d8d-b8f1-0513ef5aaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230941f-5aad-4763-b021-ebd117d4dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3d02a-ff79-4653-b64c-bc6ede0bc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c pytorch torchtext==0.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897990ea-bf8f-4f84-9b30-beb1c602c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_kor(text):\n",
    "    \"\"\"한국어를 tokenizer해서 단어들을 리스트로 만듦\"\"\"\n",
    "    return [text_ for text_ in tokenizer.morphs(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"영어를 split tokenizer해서 단어들을 리스트로 만듦\"\"\"\n",
    "    return [text_ for text_ in text.split()]\n",
    "\n",
    "# 필드 정의(train)\n",
    "\n",
    "SRC =data.Field(tokenize = tokenize_kor,\n",
    "                init_token = '<sos>',\n",
    "                eos_token = '<eos>',batch_first = True,lower = True)\n",
    "\n",
    "TRG =data.Field(tokenize = tokenize_en,\n",
    "                init_token = '<sos>',\n",
    "                eos_token = '<eos>',batch_first = True,\n",
    "                lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18cae44c-bf5e-4a38-a49c-406755d1a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled=p_data.sample(frac=1).reset_index(drop=True)\n",
    "test_shuffled = test_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e255ceeb-3fc0-43ec-8af8-a077e86691b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c876b3-8ef0-49cb-993c-42ca527c6074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200307, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "381b48d1-c1de-4aae-a3b9-78a1fba14740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150038, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8d6af7a-df0e-4884-9d96-aedad6267dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train \n",
    "from sklearn.model_selection import KFold\n",
    "df_ = df_shuffled[:100000]\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "for i,(trn_idx,val_idx) in enumerate(kf.split(df_['ko'])):\n",
    "    trn = df_.iloc[trn_idx]\n",
    "    val = df_.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b3c99858-1f46-40f2-aaa1-f21faccb4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_ = test_shuffled[:50000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "413b4dfd-e83e-4f92-ae86-9bedbbbed4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size :  100000\n",
      "trn size:  80000\n",
      "val size:  20000\n",
      "val size:  50000\n"
     ]
    }
   ],
   "source": [
    "print('df size : ' ,len(df_))\n",
    "print('trn size: ',len(trn))\n",
    "print('val size: ',len(val))\n",
    "print('val size: ',len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "242a63d1-715c-405e-9691-fb7c90c31b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "trn.to_csv(path + 'trn.csv',index = False)\n",
    "val.to_csv(path + 'val.csv',index = False)\n",
    "test_.to_csv(path + 'test.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f4fad6f-f6e9-4653-a24d-703f2908fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import TabularDataset\n",
    "\n",
    "train_data, validation_data =TabularDataset.splits(\n",
    "     path='./data/', train='trn.csv',validation= 'val.csv', format='csv',\n",
    "        fields=[('ko', SRC), ('en', TRG)], skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44e066be-aa17-437d-9f04-920b235dabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TabularDataset(path='./data/test.csv', format='csv',fields=[('ko', SRC), ('en', TRG)], skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "311f63ea-5070-43d4-92ef-7cc6eed0a643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'want', 'to', 'be', 'away', 'from', 'everyone', 'so', 'that', 'i', 'can', 'think', 'about', 'some', 'issues', '.']\n",
      "['이런저런', '생각을', '할', '수', '있도록', '모든', '사람과', '떨어져', '있고', '싶습니다.']\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[30])['ko'])\n",
    "print(vars(train_data.examples[30])['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da97b355-8b5c-479d-a631-be196f0247e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'i', \"'\", 'm', 'bbb', 'company', \"'\", 's', 'aaa', '.']\n",
      "['안녕하세요,', 'bbb', '회사', 'aaa입니다.']\n"
     ]
    }
   ],
   "source": [
    "print(vars(test_data.examples[30])['ko'])\n",
    "print(vars(test_data.examples[30])['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6096264f-91a5-4e67-b644-34f0d7a4ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(SRC): 12789\n",
      "len(TRG): 34829\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"len(SRC): {len(SRC.vocab)}\")\n",
    "print(f\"len(TRG): {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2dfc9aae-62bd-4ea6-97f5-18b1ef3b70e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(SRC): 11153\n",
      "len(TRG): 27099\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(test_data, min_freq = 2)\n",
    "TRG.build_vocab(test_data, min_freq = 2)\n",
    "\n",
    "print(f\"len(SRC): {len(SRC.vocab)}\")\n",
    "print(f\"len(TRG): {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "203ded60-dfc6-4551-ab92-d85f20ceca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "748809c7-decc-4e43-9e10-1ac52eeb0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 로더와 유사한 기능\n",
    "\n",
    "from torchtext.legacy.data import Iterator\n",
    "train_iterator = Iterator(dataset = train_data, batch_size = BATCH_SIZE)\n",
    "valid_iterator = Iterator(dataset = validation_data, batch_size = BATCH_SIZE)\n",
    "train_iterator = Iterator(dataset = test_data, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d876353-d511-4f69-84c9-ae0c561b0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    encoder와 decoder의 multi head attention 부분\n",
    "\n",
    "    임베딩 된 sequence + positional encoding (or 이전 layer의 output) 을 이용해 \n",
    "    self attention 을 수행하고 다음 layer(residual, normalization)로 보냄\n",
    "\n",
    "    output: [batch size, seq_len, hidden_size] -> input과 차원이 같이야 여러 layer를 쌓을수 있음.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % n_heads == 0\n",
    "\n",
    "        self.hidden_dim = hidden_dim # 임베딩 차원 , 논문에서는 512\n",
    "        self.n_heads = n_heads # 헤드(head)의 개수: 서로 다른 어텐션(attention) 컨셉의 수, 논문에서는 8\n",
    "        self.head_dim = hidden_dim // n_heads # 각 헤드(head)에서의 임베딩 차원 -> 각 헤드의 차원 * 헤드의 hidden dim  = input hidden dim 이여야함, 논문에서는 512/8 = 64\n",
    "\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 FC 레이어\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key 값에 적용될 FC 레이어\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 FC 레이어\n",
    "\n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim) # 512,512\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query,key, value, mask = None):\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "        # query, key, value 행렬들의 dim을 query len(sequnece size) , embeding_size(hidden_dim) 으로 초기화 함 \n",
    "\n",
    "        # query : [batch_size, query_len, hidden_dim]\n",
    "        # key : [batch_size, query_len, hidden_dim]\n",
    "        # value : [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        # Q: [batch_size, query_len, hidden_dim]\n",
    "        # K: [batch_size, key_len, hidden_dim]\n",
    "        # V: [batch_size, value_len, hidden_dim]\n",
    "\n",
    "        # multihead-attention레이어 안에서 scaled Dot-Product attetion 레이어에서 나온 Q,K,V 들을 concat함. \n",
    "        # concat한 것의 출력은 sequence_size x (n_heads x head_dim)\n",
    "        # hidden_dim → n_heads X head_dim 형태로 변형\n",
    "        \n",
    "        # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Q: [batch_size, n_heads, query_len, head_dim]\n",
    "        # K: [batch_size, n_heads, key_len, head_dim]\n",
    "        # V: [batch_size, n_heads, value_len, head_dim]\n",
    "\n",
    "        # Attention Energy 계산\n",
    "        # QK^T / self.scale\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale # 기울기 소실 방지\n",
    "\n",
    "        # energy: [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        # 마스크(mask)를 사용하는 경우\n",
    "        if mask is not None:\n",
    "            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기\n",
    "            energy = energy.masked_fill(mask==0, -1e10)\n",
    "\n",
    "        # 어텐션(attention) 스코어 계산: 각 단어에 대한 확률 값\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        # attention: [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        # 여기에서 Scaled Dot-Product Attention을 계산\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        # x: [batch_size, n_heads, query_len, head_dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # x: [batch_size, query_len, n_heads, head_dim]\n",
    "        # hidden_dim → n_heads X head_dim 형태로 변형됨 \n",
    "\n",
    "        x = x.view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        # x: [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        # x: [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d22baab-5844-4d2b-9148-73bd3bda4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "\n",
    "        # x: [batch_size, seq_len, pf_dim]\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d0e868e-5a5d-4859-b360-b9042a69162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim) # multi-head-attention레이어위에 norm 레이어 \n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        # self attention\n",
    "        # 필요한 경우 마스크(mask) 행렬을 이용하여 어텐션(attention)할 단어를 조절 가능\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask) # 하나의 입력값을 통해 query, key, value 값 생성 , decoder는 다름.\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        # position-wise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51f996fb-d86a-41c9-a494-58d32c126cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        #논문에서는 n_layers , 즉 레이어의 개수 6개 \n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)# positional encoding \n",
    "        # layer를 쌓음\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos: [batch_size, src_len]\n",
    "\n",
    "        # 소스 문장의 임베딩과 위치 임베딩을 더한 것을 사용\n",
    "        # positional encoding \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        return src # 마지막 레이어의 출력을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0628b616-95bf-4f30-ba80-12d4db99be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim) # 디코더의 첫 번째 어텐션 레이어 norm\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim) # 디코더의 두 번째 어텐션(인코더의 출력을 입력으로) 레이어 norm\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim) # 디코더의 feedforward layer norm \n",
    "        \n",
    "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        \n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        \n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "        # trg_mask: [batch_size, trg_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        # self attention\n",
    "        # 자기 자신에 대하여 어텐션(attention)\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # encoder attention\n",
    "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0bda618c-74d9-4dcd-9cc0-410f29a8507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim) # postional encoding \n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # trg: [batch_size, trg_len]\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "        # trg_mask: [batch_size, trg_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos: [batch_size, trg_len]\n",
    "\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # 소스 마스크와 타겟 마스크 모두 사용\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a10d59ac-3f31-4198-b01d-23f9dc53ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    # 소스 문장의 <pad> 토큰에 대하여 마스크(mask) 값을 0으로 설정\n",
    "    def make_src_mask(self, src):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    # 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용\n",
    "    def make_trg_mask(self, trg):\n",
    "\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        \"\"\" (마스크 예시)\n",
    "        1 0 0 0 0\n",
    "        1 1 0 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 0 0\n",
    "        \"\"\"\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        \"\"\" (마스크 예시)\n",
    "        1 0 0 0 0\n",
    "        1 1 0 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 1 0\n",
    "        1 1 1 1 1\n",
    "        \"\"\"\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "\n",
    "        # trg_sub_mask: [trg_len, trg_len]\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "69492e79-cbaf-403b-903c-ae76f5dc5e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HIDDEN_DIM = 256\n",
    "ENC_LAYERS = 6\n",
    "DEC_LAYERS = 6\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "320e4b3e-5845-4312-a502-cc516b2249e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
    "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
    "\n",
    "# Transformer 객체 선언\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7daf0079-ffdb-40c9-a529-d5a6e4f12ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(11153, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(27099, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=27099, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 초기화\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0734c95f-dc6b-4c7f-aba4-874cea41c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Adam optimizer로 학습 최적화\n",
    "LEARNING_RATE = 0.0005\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "650868af-40cc-4fba-9143-7079d3391386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습(train) 함수\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train() # 학습 모드\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # 전체 학습 데이터를 확인하며\n",
    "    for i, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.ko.to(device)\n",
    "        trg = batch.en.to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
    "        # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "        # output: [배치 크기, trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기, trg_len]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        # 출력 단어의 인덱스 0(<sos>)은 제외\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기 * trg len - 1]\n",
    "\n",
    "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward() # 기울기(gradient) 계산\n",
    "\n",
    "        # 기울기(gradient) clipping 진행\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 전체 손실 값 계산\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a0f0da5-8ef4-4442-8d56-d031d701282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가(evaluate) 함수\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval() # 평가 모드\n",
    "    epoch_loss = 0\n",
    "\n",
    "\n",
    "    # 전체 평가 데이터를 확인하며\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.ko.to(device)\n",
    "        trg = batch.en.to(device)\n",
    "\n",
    "        # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
    "        # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "        # output: [배치 크기, trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기, trg_len]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        # 출력 단어의 인덱스 0(<sos>)은 제외\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기 * trg len - 1]\n",
    "\n",
    "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        # 전체 손실 값 계산\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "09322a15-dce4-4cd5-a981-98e321712bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e9c96a14-d195-4714-8df7-67bfe14585a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 34.31s | valid loss  8.88 |  train loss |  4.97 | valid ppl  7154.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 34.76s | valid loss  8.87 |  train loss |  4.97 | valid ppl  7120.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 34.61s | valid loss  8.87 |  train loss |  4.96 | valid ppl  7134.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 34.51s | valid loss  8.86 |  train loss |  4.97 | valid ppl  7060.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 34.53s | valid loss  8.85 |  train loss |  4.96 | valid ppl  6976.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 34.36s | valid loss  8.87 |  train loss |  4.96 | valid ppl  7080.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 35.55s | valid loss  8.87 |  train loss |  4.96 | valid ppl  7098.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 34.46s | valid loss  8.86 |  train loss |  4.96 | valid ppl  7024.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 34.48s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7220.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 34.49s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7152.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 34.59s | valid loss  8.87 |  train loss |  4.96 | valid ppl  7147.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 34.47s | valid loss  8.86 |  train loss |  4.96 | valid ppl  7076.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 35.74s | valid loss  8.87 |  train loss |  4.96 | valid ppl  7130.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 34.70s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7153.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 34.37s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7171.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 34.39s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7177.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 34.35s | valid loss  8.87 |  train loss |  4.96 | valid ppl  7115.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 34.36s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7182.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 34.63s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7168.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 35.27s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7187.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 35.50s | valid loss  8.87 |  train loss |  4.96 | valid ppl  7119.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 35.51s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7255.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 35.46s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7176.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 35.32s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7184.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 35.09s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7224.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 35.21s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7162.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 35.34s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7159.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 35.49s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7195.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 35.53s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7170.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 35.52s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7251.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 35.57s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7177.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 35.58s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7193.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 35.64s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7218.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 35.64s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7179.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 35.61s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7205.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 35.62s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7205.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 35.65s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7211.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 35.54s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7214.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 35.51s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7215.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 35.48s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7218.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 35.54s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7197.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 35.55s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7213.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 35.56s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7175.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 35.77s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7170.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 35.41s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7210.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 35.68s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7185.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 35.80s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7231.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 35.68s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7188.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 35.60s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7167.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 35.67s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7176.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 35.56s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7229.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 35.67s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7194.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 35.49s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7208.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 35.67s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7203.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 35.50s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7221.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 35.59s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7181.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 35.48s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7199.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 35.64s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7223.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 35.43s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7209.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 35.45s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7213.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 35.38s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7211.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 35.33s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7185.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 35.24s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7221.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 35.27s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7194.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 35.43s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7216.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 35.26s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7213.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 35.21s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7243.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 35.20s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7228.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 35.15s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7195.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 35.47s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7221.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 35.26s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7213.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 35.37s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7200.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 35.31s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7228.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 35.39s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7213.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 35.36s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7214.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 35.45s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7216.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 35.50s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7199.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 35.51s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7221.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 35.43s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7194.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 35.34s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7202.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 35.28s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7205.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 35.23s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7245.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 35.27s | valid loss  8.88 |  train loss |  4.95 | valid ppl  7219.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 35.18s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7216.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 35.17s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7216.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 35.19s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7208.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 35.07s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7229.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 35.25s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7228.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 35.15s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7246.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 35.31s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7234.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 35.41s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7223.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 35.40s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7235.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 35.22s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7221.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 35.30s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7217.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 35.22s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7234.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 35.24s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7223.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 35.43s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7233.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 35.46s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7219.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 35.39s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7199.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time: 35.46s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7232.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 101 | time: 35.19s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7229.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 102 | time: 35.41s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7218.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 103 | time: 35.38s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7220.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 104 | time: 35.23s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7218.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 105 | time: 35.44s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7224.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 106 | time: 35.32s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7219.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 107 | time: 35.38s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7234.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 108 | time: 35.45s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7234.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 109 | time: 35.46s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7220.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 110 | time: 35.47s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7229.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 111 | time: 35.34s | valid loss  8.89 |  train loss |  4.95 | valid ppl  7238.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 112 | time: 35.45s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7231.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 113 | time: 35.46s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7210.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 114 | time: 35.41s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7242.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 115 | time: 35.45s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7228.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 116 | time: 35.42s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7218.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 117 | time: 35.22s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7216.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 118 | time: 35.22s | valid loss  8.89 |  train loss |  4.95 | valid ppl  7231.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 119 | time: 35.31s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7215.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 120 | time: 35.31s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7210.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 121 | time: 35.50s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7189.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 122 | time: 34.62s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7225.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 123 | time: 34.35s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7226.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 124 | time: 34.39s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7223.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 125 | time: 35.46s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7218.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 126 | time: 35.65s | valid loss  8.88 |  train loss |  4.95 | valid ppl  7220.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 127 | time: 35.60s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7203.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 128 | time: 35.46s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7193.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 129 | time: 35.31s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7212.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 130 | time: 35.15s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7219.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 131 | time: 35.28s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7220.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 132 | time: 35.42s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7221.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 133 | time: 35.76s | valid loss  8.89 |  train loss |  4.95 | valid ppl  7227.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 134 | time: 35.54s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7187.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 135 | time: 35.54s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7224.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 136 | time: 35.63s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7219.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 137 | time: 35.75s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7222.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 138 | time: 35.64s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7204.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 139 | time: 35.53s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7224.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 140 | time: 35.53s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7198.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 141 | time: 35.53s | valid loss  8.88 |  train loss |  4.95 | valid ppl  7204.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 142 | time: 35.58s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7210.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 143 | time: 35.51s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7230.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 144 | time: 35.62s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7212.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 145 | time: 35.63s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7229.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 146 | time: 35.64s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7212.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 147 | time: 35.58s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7216.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 148 | time: 35.73s | valid loss  8.88 |  train loss |  4.95 | valid ppl  7201.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 149 | time: 35.69s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7223.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 150 | time: 35.62s | valid loss  8.88 |  train loss |  4.95 | valid ppl  7210.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 151 | time: 35.46s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7222.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 152 | time: 35.72s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7227.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 153 | time: 35.68s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7223.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 154 | time: 35.63s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7222.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 155 | time: 35.63s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7205.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 156 | time: 35.65s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7217.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 157 | time: 35.66s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7210.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 158 | time: 35.59s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7230.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 159 | time: 35.55s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7226.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 160 | time: 35.48s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7258.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 161 | time: 35.59s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7218.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 162 | time: 35.73s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7227.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 163 | time: 35.67s | valid loss  8.89 |  train loss |  4.95 | valid ppl  7233.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 164 | time: 35.41s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7229.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 165 | time: 35.50s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7221.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 166 | time: 35.36s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7211.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 167 | time: 35.44s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7209.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 168 | time: 35.28s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7215.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 169 | time: 35.25s | valid loss  8.89 |  train loss |  4.95 | valid ppl  7225.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 170 | time: 35.44s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7215.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 171 | time: 35.42s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7214.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 172 | time: 35.35s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7213.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 173 | time: 35.31s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7224.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 174 | time: 35.40s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7211.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 175 | time: 35.46s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7209.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 176 | time: 35.43s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7251.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 177 | time: 35.34s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7222.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 178 | time: 35.37s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7217.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 179 | time: 35.29s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7236.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 180 | time: 35.51s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7211.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 181 | time: 35.41s | valid loss  8.89 |  train loss |  4.95 | valid ppl  7235.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 182 | time: 35.43s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7225.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 183 | time: 35.46s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7229.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 184 | time: 35.52s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7212.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 185 | time: 35.33s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7253.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 186 | time: 35.21s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7217.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 187 | time: 35.24s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7212.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 188 | time: 35.26s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7242.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 189 | time: 35.38s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7231.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 190 | time: 35.19s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7211.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 191 | time: 35.28s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7203.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 192 | time: 35.23s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7239.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 193 | time: 35.21s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7222.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 194 | time: 35.21s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7223.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 195 | time: 35.31s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7214.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 196 | time: 35.49s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7205.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 197 | time: 35.50s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7237.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 198 | time: 35.43s | valid loss  8.89 |  train loss |  4.95 | valid ppl  7227.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 199 | time: 35.50s | valid loss  8.89 |  train loss |  4.96 | valid ppl  7228.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 200 | time: 35.30s | valid loss  8.88 |  train loss |  4.96 | valid ppl  7194.04\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "import copy \n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs = 200\n",
    "best_model = None\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = [] \n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    \n",
    "    val_ppl = math.exp(valid_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | ' f'valid loss {valid_loss:5.2f} |  train loss | {train_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if valid_loss < best_val_loss:\n",
    "        best_val_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'transformer_en_to_ko.pt')\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "865c87d8-a0ac-42f0-80ab-2b35502f815b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21e8e52ff10>]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7b2beeee-b964-4ef0-9cf1-4662552c9674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  8.40 | test ppl  4433.05\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, valid_iterator, criterion)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a7cd3c-e189-4125-ba47-dbdf7aac0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time() # 시작 시간 기록\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time() # 종료 시간 기록\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'transformer_en_to_ko.pt')\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33249aba-0f2b-4d25-b0d6-51a4bfb12798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역(translation) 함수\n",
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50, logging=True):\n",
    "    model.eval() # 평가 모드\n",
    "\n",
    "    if isinstance(sentence, str):        \n",
    "        tokens = [text_ for text_ in tokenizer.morphs(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
    "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
    "\n",
    "    # text -> token\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    # 소스 문장에 따른 마스크 생성\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    # 인코더(endocer)에 소스 문장을 넣어 출력 값 구하기\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
    "    trg_indexes = [trg_field.vocab.stoi['<sos>']]\n",
    "\n",
    "    # 한 단어씩 생성\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        # 출력 문장에 따른 마스크 생성\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # 출력 문장에서 가장 마지막 단어만 사용\n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
    "\n",
    "        # <eos>를 만나는 순간 끝\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a688901-0187-49cf-b469-f52dea79eb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I think it startd from that time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 번역 결과: 그 시기에 그렇게 하는 것 같네요. <eos>\n",
      "attention :  torch.Size([1, 8, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "example_idx = 10\n",
    "\n",
    "src = input()\n",
    "\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device, logging=True)\n",
    "# trg: [batch_size, trg_len, hidden_dim]\n",
    "# attention: [batch_size, n_heads, trg_len, src_len]\n",
    "print('')\n",
    "print(\"모델 번역 결과:\", \" \".join(translation))\n",
    "print(\"attention : \", attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c9b0f-8c68-4f67-ae29-d95935cc7f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
